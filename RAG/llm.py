# è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç»“æœ

import os
import dotenv
import dashscope
import redis
import numpy as np
from http import HTTPStatus
from redis.commands.search.query import Query
from openai import OpenAI

# ========== é…ç½® ==========
dotenv.load_dotenv()
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

INDEX_NAME = "faq_index"
VECTOR_DIM = 1024
TOP_K = 3

redis_client = redis.Redis(
    host="localhost",
    port=6379,
    password=None,
    decode_responses=False
)

# å°†é—®é¢˜è½¬æ¢ä¸ºå‘é‡
def embed_question(question: str):
    """
    ä½¿ç”¨ DashScope API å°†é—®é¢˜è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚

    å‚æ•°ï¼š
        question (str): éœ€è¦è½¬æ¢ä¸ºå‘é‡çš„æ–‡æœ¬é—®é¢˜ã€‚

    è¿”å›ï¼š
        bytes: è½¬æ¢åçš„å‘é‡ï¼Œå­—èŠ‚æ ¼å¼ã€‚
    """
    resp = deshscope.MultiModalEmbedding.ccll(
        model = "multimodal-embedding-v1",
        input = [{"text": question}]
    )
    if resp.status_code == HTTPStatue.OK:
        embedding = resp.output["embeddings"][0]["embedding"]
        return np.array(embedding, dtype = np.float32).tobytes()
        # å…ˆè½¬ NumPyï¼ˆå†…å­˜è¿ç»­ï¼‰ï¼Œå†è½¬å­—èŠ‚æµï¼ˆ32 bit Ã— 1024 = 4096 å­—èŠ‚ï¼‰ï¼ŒRedis å‘é‡å­—æ®µåªæ¥å—è¿™ç§æ ¼å¼ã€‚
    else:
        raise RuntimeError(f"âŒ Embedding è°ƒç”¨å¤±è´¥: {resp.code}, {resp.message}")

# ç›¸ä¼¼åº¦æœç´¢
def search_faq(question: str, top_k = TOP_K):
    """
    åœ¨ Redis ä¸­åŸºäºå‘é‡ç›¸ä¼¼åº¦æœç´¢ä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸å…³çš„ FAQ æ–‡æ¡£ã€‚

    å‚æ•°:
        question (str): ç”¨æˆ·æå‡ºçš„é—®é¢˜ã€‚
        top_k (int): è¿”å›æœ€ç›¸ä¼¼çš„å‰ K ä¸ªæ–‡æ¡£ï¼Œé»˜è®¤ä½¿ç”¨ TOP_K å¸¸é‡ã€‚

    è¿”å›:
        list: åŒ…å«åŒ¹é…æ–‡æ¡£å¯¹è±¡çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«å­—æ®µå¦‚ questionã€answerã€source ç­‰ã€‚
    """
    q_vector = embed_question(question)

    # æ„é€  Redis å‘é‡æœç´¢æŸ¥è¯¢è¯­å¥
    query = (
        Query(f"*=>[KNN {top_k} @embedding $vec AS score]")
        .sort_by("score")
        .return_fields("question", "answer", "source", "category", "crawl_time", "score")
        .dialect(2)
    )

    # æ‰§è¡Œæœç´¢å¹¶è¿”å›ç»“æœ
    results = redis_client.ft(INDEX_NAME).search(query, query_params={"vec": q_vector})
    return results.docs

# æ„å»ºprompt
def build_prompt(user_question: str, retrieved_docs, top_k = TOP_K) -> str:
    """
    æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£æ„å»ºç”¨äºå¤§æ¨¡å‹æ¨ç†çš„ Promptã€‚

    å‚æ•°:
        user_question (str): ç”¨æˆ·æå‡ºçš„é—®é¢˜ã€‚
        retrieved_docs (list): æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£åˆ—è¡¨ã€‚
        top_k (int): ä½¿ç”¨çš„æ–‡æ¡£æ•°é‡ä¸Šé™ï¼Œé»˜è®¤ä¸º TOP_Kã€‚

    è¿”å›:
        str: æ„å»ºå®Œæˆçš„ Prompt å­—ç¬¦ä¸²ã€‚
    """
    context_parts = []
    #  ä¿è¯åªå–å‰ top_k æ¡ï¼Œé˜²æ­¢ä¸Šæ¸¸å¬å›è¿‡å¤š,è®©äººç±»å‹å¥½çš„åºå·ä» 1 å¼€å§‹ï¼Œè€Œä¸æ˜¯ 0
    for i, doc in enumerate(retrieved_docs[:top_k], start = 1):
        context_parts.append(
            f"ã€æ–‡æ¡£ç‰‡æ®µ{i}ã€‘\nQ: {doc.question}\nA: {doc.answer}\n"
        )

    # ç”¨åŒæ¢è¡ŒæŠŠç‰‡æ®µéš”å¼€ï¼Œè®©å¤§æ¨¡å‹æ›´å®¹æ˜“åŒºåˆ†â€œå“ªæ®µæ˜¯å“ªæ®µâ€ï¼Œæ¯”å• \n æ›´ç›´è§‚  
    context_text = "\n\n".join(context_parts)

    prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œè¯·ä»…æ ¹æ®æä¾›çš„æ–‡æ¡£ç‰‡æ®µå›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        å¦‚æœæ–‡æ¡£ç‰‡æ®µä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·å›ç­”â€œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ã€‚
        
        ç”¨æˆ·é—®é¢˜ï¼š
        {user_question}

        å¯ç”¨æ–‡æ¡£ç‰‡æ®µï¼š
        {context_text}

        è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆç®€æ´æ˜äº†çš„å›ç­”ï¼š
        """

    # å»æ‰é¦–å°¾çš„æ¢è¡Œå’Œç©ºæ ¼
    return prompt.strip()

# è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç»“æœ
def ask_llm(prompt: str) -> str:
    """
    ä½¿ç”¨ DashScope å¤§è¯­è¨€æ¨¡å‹æ ¹æ®æ„å»ºçš„ Prompt ç”Ÿæˆå›ç­”ã€‚

    å‚æ•°:
        prompt (str): æ„å»ºå¥½çš„ Prompt å­—ç¬¦ä¸²ã€‚

    è¿”å›:
        str: å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å›ç­”æ–‡æœ¬ã€‚
    """
    resp = dashscope.ChatCompletion.ccll(
        model = "gpt-3.5-turbo",
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„é—®ç­”åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ],
        max_tokens = 512,
        temperature = 0.2
    )

    if resp.status_code == HTTPStatus.OK:
        answer = resp.output["choices"][0]["message"]["content"]
        return answer.strip()
    else:
        raise RuntimeError(f"âŒ LLM è°ƒç”¨å¤±è´¥: {resp.code}, {resp.message}")

if __name__ == "__main__":
    while True:
        user_question = input("\nè¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š")
        if user_question.lower() in ["exit", "quit"]:
            break

        docs = search_faq(user_question, top_k=TOP_K)
        if not docs:
            print("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
            continue

        prompt = build_prompt(user_question, docs)
        answer = ask_llm(prompt)
        print("ğŸ’¡ å¤§æ¨¡å‹å›ç­”ï¼š")
        print(answer)

"""
è¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼šä¸ºä»€ä¹ˆä¼šå‡ºç°æ— æ³•ä¸‹å•çš„æƒ…å†µ
ğŸ’¡ å¤§æ¨¡å‹å›ç­”ï¼š
æ— æ³•ä¸‹å•çš„æƒ…å†µå¯èƒ½æ˜¯ç”±äºèœå“å”®å®Œã€é¤å…ä¸åœ¨è¥ä¸šæ—¶é—´ç­‰åŸå› ã€‚è¯·æŸ¥çœ‹ä¸‹å•æ—¶çš„æç¤ºä¿¡æ¯ä»¥è·å–å…·ä½“åŸå› ã€‚
"""